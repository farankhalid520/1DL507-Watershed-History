{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python version 3.10.13\n",
    "# Author : Venugopal\n",
    "\n",
    "import folium\n",
    "m = folium.Map()\n",
    "m = folium.Map(location=(60.63879, 15.44528))\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "lake_chem = pd.read_csv('C:/Users/vijay/Documents/projects/dataScienceProject/lakechemdataclean.csv')\n",
    "lake_chem_lat_long = lake_chem.drop_duplicates(subset=['Latitude', 'Longitude'],keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add marker one by one on the map to locate the lake collection points\n",
    "\n",
    "for i in range(0,len(lake_chem_lat_long)):\n",
    "   folium.Marker(\n",
    "      location=[lake_chem_lat_long.iloc[i]['Latitude'], lake_chem_lat_long.iloc[i]['Longitude']],\n",
    "      popup=lake_chem_lat_long.iloc[i]['Survey station'],\n",
    "   ).add_to(m)\n",
    "\n",
    "m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Extracting the longitude and latitude from the files to create geometry of the shapefile for that particular year and season\n",
    "df_y05s01 = pd.read_csv(r\"C:\\Users\\vijay\\Documents\\projects\\dataScienceProject\\data\\year_season\\y05_s01.csv\") \n",
    "df_y05s01_shp = gpd.GeoDataFrame(df_y05s01, crs='EPSG:4326', geometry=gpd.points_from_xy(df_y05s01['Longitude'], df_y05s01['Latitude']))\n",
    "\n",
    "# saving the created shapefile in the destination directory\n",
    "filepath = r\"C:/Users/vijay/Documents/projects/dataScienceProject/data/year_season/shapefiles\"\n",
    "df_y05s01_shp.to_file(f\"{filepath}/y05s01.shp\", driver='ESRI Shapefile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "#Alternative way to achieve the same results\n",
    "geometry = [Point(xy) for xy in zip(df_landscape['Longitude'], df_landscape['Latitude'])]\n",
    "gdf = GeoDataFrame(df_landscape, geometry=geometry)\n",
    "gdf.set_crs(epsg=4326, inplace=True)\n",
    "gdf.to_crs(epsg=3395)\n",
    "\n",
    "gdf\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Data processing for clipping according to the year and season\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\vijay\\Documents\\projects\\dataScienceProject\\lakechemdataclean.csv\")\n",
    "df.head()\n",
    "\n",
    "y22_s04 = df.loc[(df['Year'] == 2022) & (df['Season'] == 4)]\n",
    "y22_s04.to_csv(r\"C:\\Users\\vijay\\Documents\\projects\\dataScienceProject\\data\\year_season\\y22_s04.csv\")\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "# Cleaning the data for NaN or Inf\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(r\"E:\\year_season\\y20_s04.csv\")\n",
    "df.columns[df.isna().any()].tolist()\n",
    "\n",
    "# Dropping the columns which contain NaN values\n",
    "drop_columns = ['cpyll', 'oxy', 'water_temp']\n",
    "df = df.drop(drop_columns, axis=1)\n",
    "\n",
    "\n",
    "# Saving the cleaned file to destination location\n",
    "filepath=r\"E:/\"\n",
    "df.to_csv(f\"{filepath}/y20s04.csv\")\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the lake chemistry data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r\"E:\\year_season\\y05_s01.csv\")\n",
    "#df.info()\n",
    "\n",
    "df = df.drop(['md_mvm_id',\n",
    "              'lat','long',\n",
    "              'year', 'month', 'day',\n",
    "              'season_type', 'season',\n",
    "              'max_sample_depth'],\n",
    "               axis=1)\n",
    "\n",
    "df_ = df.fillna(df.mean())\n",
    "df_normalized = (df_ - df_.mean())/df_.std()  \n",
    "\n",
    "df_normalized.isnull().any(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing and data manipulation for PCA\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r\"E:\\year_season_copy\\y05s01_shapefile\\y05_s01.csv\")\n",
    "#df.head()\n",
    "df_copy = df.copy()\n",
    "\n",
    "df_for_normalization = df_copy.drop(['md_mvm_id','lat','long','year', 'month','day','season_type','season'], axis=1)\n",
    "\n",
    "df = df_for_normalization.fillna(df_for_normalization.mean())\n",
    "df_ = df.drop(['toc', 'abs420'], axis=1)\n",
    "df_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized = (df_ - df_.mean()) / df_.std()\n",
    "df_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plot\n",
    "\n",
    "pca = PCA(n_components=10)\n",
    "pca.fit(df_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_\n",
    "pca_df = pd.DataFrame(pca.components_)\n",
    "pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=df_.shape[1])\n",
    "pca.fit(df_normalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat and view results\n",
    "\n",
    "loadings = pandas.DataFrame(pca.components_.T,\n",
    "columns=['PC%s' % _ for _ in range(len(df_normalized.columns))],\n",
    "index=df_.columns)\n",
    "print(loadings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the variance vs components\n",
    "\n",
    "plot.plot(pca.explained_variance_ratio_)\n",
    "plot.ylabel('Explained Variance')\n",
    "plot.xlabel('Components')\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pca import pca\n",
    "\n",
    "model = pca(n_components=0.95)\n",
    "results = model.fit_transform(df_normalized)\n",
    "fig, ax = model.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bi-plot\n",
    "model.biplot(cmap=None, legend=1, n_feat=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the principle component loadings\n",
    "print(model.results['topfeat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest Implementation part\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "path_to_data = r\"E:\\year_season_copy\\y05s01_shapefile\\y05_s01.csv\"\n",
    "dataset = pd.read_csv(path_to_data)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data Cleaning and normalizing\n",
    "\n",
    "drop_columns  =['md_mvm_id','lat','long','abs420','year','month', 'day', 'season_type', 'season']\n",
    "dataset = dataset.drop(drop_columns, axis=1)\n",
    "\n",
    "\n",
    "data_median = dataset.median()\n",
    "dataset = dataset.fillna(data_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation in the data\n",
    "\n",
    "dataset.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the heatmap\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "sns.set(style=\"white\")\n",
    "corr = dataset.corr()\n",
    "mask = np.zeros_like(corr, dtype=bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "f, ax = plt.subplots(figsize=(20, 19))\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for NaN values before modelling\n",
    "\n",
    "dataset.isna().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning the Dependent and Independent variables\n",
    "\n",
    "labels = ['toc']\n",
    "y = dataset[labels].values\n",
    "X = dataset.drop(labels, axis=1).values\n",
    "\n",
    "y.shape, X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scl_X = RobustScaler().fit(X_train)\n",
    "scl_y = RobustScaler().fit(y_train)\n",
    "input_x_train = scl_X.transform(X_train)\n",
    "input_y_train = scl_y.transform(y_train)\n",
    "input_x_test = scl_X.transform(X_test)\n",
    "input_y_test = scl_y.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.multioutput import MultiOutputRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = MultiOutputRegressor(LinearRegression())\n",
    "linreg.fit(input_x_train, input_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training score for linear regression\n",
    "linreg.score(input_x_train, input_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing score for linear regression\n",
    "linreg.score(input_x_test, input_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random forest modelling\n",
    "RF = MultiOutputRegressor(RandomForestRegressor(n_estimators=200, max_features=\"sqrt\", max_depth=10, min_samples_leaf=7))\n",
    "RF.fit(input_x_train, input_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "RF.score(input_x_train, input_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "RF.score(input_x_test, input_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Regressor implementation part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor   #algorithm\n",
    "from sklearn.ensemble import RandomForestClassifier   #algorithm\n",
    "from sklearn.ensemble import GradientBoostingClassifier   #algorithm\n",
    "\n",
    "from sklearn.model_selection import train_test_split   #split dataset to train and test\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "import matplotlib.pyplot as plt   #for regurlar plots\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#settings\n",
    "%matplotlib inline\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build model\n",
    "tree = DecisionTreeRegressor(random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_depth': [80, 90, 100, None],\n",
    "    'max_features': [\"sqrt\", \"log2\", None],\n",
    "    'min_samples_leaf': [1, 3, 5],\n",
    "    'min_samples_split': [2, 3, 4],\n",
    "    'criterion': [\"squared_error\", \"friedman_mse\", \"absolute_error\", \"poisson\"],\n",
    "    'splitter': [\"best\", \"random\"]\n",
    "}\n",
    "\n",
    "rf = DecisionTreeRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 5, verbose =2, scoring='r2',  n_jobs = -1)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# searching for the best parameters to fit\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fine tuning model\n",
    "\n",
    "tree = DecisionTreeRegressor(random_state=42, criterion='absolute_error', max_depth=80, max_features=None,min_samples_leaf=5, min_samples_split=3,splitter=\"random\")\n",
    "tree.fit(X_train, y_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
